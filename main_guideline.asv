% Simple Random Forest Toolbox for Matlab
% written by Mang Shao and Tae-Kyun Kim, June 20, 2014.
% updated by Tae-Kyun Kim, Feb 09, 2017

% This is a guideline script of simple-RF toolbox.
% The codes are made for educational purposes only.
% Some parts are inspired by Karpathy's RF Toolbox

% Under BSD Licence

% Initialisation
init;

% Select dataset
[data_train, data_test] = getData('Toy_Spiral'); % {'Toy_Gaussian', 'Toy_Spiral', 'Toy_Circle', 'Caltech'}

%%
%%%%%%%%%%%%%
% check the training and testing data
    % data_train(:,1:2) : [num_data x dim] Training 2D vectors
    % data_train(:,3) : [num_data x 1] Labels of training data, {1,2,3}
    
plot_toydata(data_train);

    % data_test(:,1:2) : [num_data x dim] Testing 2D vectors, 2D points in the
    % uniform dense grid within the range of [-1.5, 1.5]
    % data_train(:,3) : N/A
    
scatter(data_test(:,1),data_test(:,2),'.b');


%% Visualize some bootstrapped bags
% Bootstraping aggregating
[N,D] = size(data_train);
frac = 1 - 1/exp(1); % Bootstrap sampling fraction: 1 - 1/e (63.2%)

figure
subplot(2,2,1)       % add first plot in 2 x 1 grid
idx = randsample(N,ceil(N),1); % A new training set for each tree is generated by random sampling from dataset WITH replacement.
plot_toydata(data_train(idx,:));
title('Subplot 1')

subplot(2,2,2)       % add first plot in 2 x 1 grid
idx = randsample(N,ceil(N),1); % A new training set for each tree is generated by random sampling from dataset WITH replacement.
plot_toydata(data_train(idx,:));
title('Subplot 2')

subplot(2,2,3)       % add first plot in 2 x 1 grid
idx = randsample(N,ceil(N),1); % A new training set for each tree is generated by random sampling from dataset WITH replacement.
plot_toydata(data_train(idx,:));
title('Subplot 3')

subplot(2,2,4)       % add first plot in 2 x 1 grid
idx = randsample(N,ceil(N),1); % A new training set for each tree is generated by random sampling from dataset WITH replacement.
plot_toydata(data_train(idx,:));
title('Subplot 4')

%%
% Set the random forest parameters for instance, 
param.num = 20;         % Number of trees
param.depth = 7;        % trees depth
param.splitNum = 5;     % Number of split functions to try
param.split = 'IG';     % Currently support 'information gain' only

%%%%%%%%%%%%%%%%%%%%%%
% Train Random Forest

% Grow all trees
trees = growTrees(data_train,param);

%% Visualize class distributions of leaf nodes
figure('pos', [100,200,2400,400]);
subplot(1,4,1);
bar(trees(1).leaf(1).prob);
axis([0.5 3.5 0 1]);
title('Histogram leaf 1, tree 1','FontSize', 26);

subplot(1,4,2);
bar(trees(1).leaf(3).prob);
axis([0.5 3.5 0 1]);
title('Histogram leaf 3, tree 1','FontSize', 26);

subplot(1,4,3);
bar(trees(1).leaf(5).prob);
axis([0.5 3.5 0 1]);
title('Histogram leaf 5, tree 1','FontSize', 26);

subplot(1,4,4);
bar(trees(1).leaf(7).prob);
axis([0.5 3.5 0 1]);
title('Histogram leaf 7, tree 1','FontSize', 26);

%%
%%%%%%%%%%%%%%%%%%%%%%
% Evaluate/Test Random Forest

% grab the few data points and evaluate them one by one by the learnt RF
% test_point = [-.5 -.7; .4 .3; -.7 .4; .5 -.5];
% for n=1:4
%     leaves = testTrees([test_point(n,:) 0],trees);
%     % average the class distributions of leaf nodes of all trees
%     p_rf_sum = sum(trees(1).prob(leaves,:));
%     p_rf_mean = p_rf_sum/length(trees);
%     p_rf_mean
% end

% Test on the dense 2D grid data, and visualise the results ... 
predicted_labels = zeros(size(data_test,1), 1);
p_rf = zeros(size(data_test,1), 3);
for n=1:size(data_test, 1)
    leaves = testTrees([data_test(n,:) 0],trees);
    % average the class distributions of leaf nodes of all trees
    p_rf_sum = sum(trees(1).prob(leaves,:));
    p_rf_mean = p_rf_sum/length(trees);
    p_rf(n,:) = p_rf_mean;
    [~, predicted_labels(n,1)] = max(p_rf_mean);
    [data_test(n,3), predicted_labels(n,1)];
end
%%
figure;
% plot_toydata([data_test(:,1:end-1), predicted_labels]);
p_all = sum(p_rf,3);
imagesc([-1.5 1.5],[-1.5 1.5],reshape(p_all,151,151,3)/size(p_rf,3));
hold on;
plot_toydata(data_train(:,:));
title(['numTrees= ' num2str(param.num) '; Depth= ' num2str(param.depth) '; splitNum= ' num2str(param.splitNum)]);
set(findall(gcf,'type','axes'),'fontsize', 12);
set(findall(gcf,'type','text'),'fontSize', 22);

% Change the RF parameter values and evaluate ... 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%% experiment with Caltech101 dataset for image categorisation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

init;

%%
% Select dataset
% we do bag-of-words technique to convert images to vectors (histogram of codewords)
% Set 'showImg' in getData.m to 0 to stop displaying training and testing images and their feature vectors
k_range = 2.^linspace(1,10,10);
ind = 1;
for k = k_range
    [data_train_not_norm, data_test_not_norm] = getData('Caltech', k);
    
    % Normalize histograms to be between 0 and 1
    data_train = data_train_not_norm;
    data_test = data_test_not_norm;
    for i = 1:size(data_train, 1)
        data_train(i,1:end-1) = data_train_not_norm(i,1:end-1) / sum(data_train_not_norm(i,1:end-1));
    end
    for i = 1:size(data_test, 1)
        data_test(i,1:end-1) = data_test_not_norm(i,1:end-1) / sum(data_test_not_norm(i,1:end-1));
    end
    
    data_train_arr{ind} = data_train;
    data_test_arr{ind} = data_test;
    ind++;

save('kmeans_codebooks.mat','data_train_arr','data_test_arr')
%%
% Visualize histograms
for i = 1:size(data_train, 1)
    bar(data_train(i,1:end-1));
    axis([0 260 0 0.2])
    title(sprintf('Class %d', data_train(i, end)));
    pause();
end
close all;

%%
% Set the random forest parameters ...
% Train Random Forest ...
% Evaluate/Test Random Forest ...
% show accuracy and confusion matrix ...

param.num = 10;         % Number of trees
param.depth = 5;        % trees depth
param.splitNum = 3;     % Number of split functions to try
param.split = 'IG';     % Currently support 'information gain' only

%%%%%%%%%%%%%%%%%%%%%%
% Train Random Forest

% Grow all trees
trees = growTrees(data_train,param);


%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% random forest codebook for Caltech101 image categorisation
% .....

% Test on the dense 2D grid data, and visualise the results ... 
predicted_labels = zeros(size(data_test,1), 1);
p_rf = zeros(size(data_test,1), 10);
for n=1:size(data_test, 1)
    leaves = testTrees([data_test(n,:) 0],trees);
    % average the class distributions of leaf nodes of all trees
    p_rf_sum = sum(trees(1).prob(leaves,:));
    p_rf_mean = p_rf_sum/length(trees);
    p_rf(n,:) = p_rf_mean;
    [~, predicted_labels(n,1)] = max(p_rf_mean);
    [data_test(n,end), predicted_labels(n,1)]
end

accuracy = sum(data_test(:,end) == predicted_labels(:,1)) / size(data_test,1)
% v = horzcat(data_test(:,end), predicted_labels(:,1), data_test(:,end) == predicted_labels(:,1));

%%
% [~,c] = max(p_rf');
[~,c] = max(p_rf');
accuracy_rf = sum(c==data_test(:,end)')/length(c); % Classification accuracy (for Caltech dataset)
idx = sub2ind([10, 10], data_test(:,end)', c) ;
conf = zeros(10) ;
conf = vl_binsum(conf, ones(size(idx)), idx) ;

imagesc(conf) ;
title(sprintf('Confusion matrix (%.2f %% accuracy)', 100 * accuracy_rf) ) ;

%% RF Codebook
[data_train, data_test] = getDataRFCodebook('Caltech');

%% RF classifier on RF codebook
param.num = 50;         % Number of trees
param.depth = 5;        % trees depth
param.splitNum = 5;     % Number of split functions to try
param.split = 'IG';     % Currently support 'information gain' only
trees = growTrees(data_train,param);

%%
% Test on the dense 2D grid data, and visualise the results ... 
predicted_labels = zeros(size(data_test,1), 1);
p_rf = zeros(size(data_test,1), 10);
for n=1:size(data_test, 1)
    leaves = testTrees([data_test(n,:) 0],trees);
    % average the class distributions of leaf nodes of all trees
    p_rf_sum = sum(trees(1).prob(leaves,:));
    p_rf_mean = p_rf_sum/length(trees);
    p_rf(n,:) = p_rf_mean;
    [~, predicted_labels(n,1)] = max(p_rf_mean);
    [data_test(n,end), predicted_labels(n,1)]
end

accuracy = sum(data_test(:,end) == predicted_labels(:,1)) / size(data_test,1)
% compare = [data_test(:,end), predicted_labels(:,1)];